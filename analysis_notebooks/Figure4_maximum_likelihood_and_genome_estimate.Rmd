---
title: "Figure 5 - Maximum Likelihood Model and Genome-wide Estimate"
author: "Patrick Short"
date: "7 December 2016"
output: html_document
---

Code to generate Figure 4 of the non-coding burden paper.

```{r use power calc to generate ML model}
library(ggplot2)
library(gridExtra)
library(stringr)

conserved_fb_active = read.table("../data/conserved_elements.min10_coverage.fb_active_roadmap_union.txt", header = TRUE, sep = "\t")

prevalence<-1/120 # fold enrichment of LOF mutations in DDD cohort
penetrance<-1 # penetrance of LoF mutations, to allow estimate of frequency of LoF mutations in cohort
num.trios<-6239
recurr=seq(2,6)
num.transmissions<-num.trios*2
thresh<-0.05/nrow(conserved_fb_active) # p value required to detect significantly mutated gene
max.DNMs<-10 # max number of DNMs to consider

n_recurr = 25
n_obs = 286 # from fetal brain active observed elements
n_signif = 0

obs_vector = c(rep(0, nrow(conserved_fb_active) - 234 - 25), rep(1, 234),rep(2,23), 3, 3)

total_obs = sum(obs_vector)
total_recurr = sum(obs_vector > 1)
total_gws = sum(obs_vector > 3)

p_data_abc = function(n_true_elements, lof.density, obs_vector) {
  
  # simulate data
  true_element_idx = sample(seq(1, nrow(conserved_fb_active)), n_true_elements)
  lof.rate = conserved_fb_active$p_snp_null * lof.density
  
  # there are n_true_elements which will have mutations at rate
  rate_parameter_true_elements = (lof.rate*num.trios/prevalence*penetrance + conserved_fb_active$p_snp_null*num.trios)[true_element_idx]
  
  # there are n - n_true elements which have mutations at rate
  rate_parameter_false_elements = (conserved_fb_active$p_snp_null*num.trios)[-true_element_idx]
  
  all_elements = c(rate_parameter_true_elements, rate_parameter_false_elements)
  
  # probability of observing a total of n_recurr recurrently mutated elements
  sim = sapply(all_elements, function(l) rpois(100, l))
  
  
  # probability of observing total DNMs
  total = rowSums(sim)
  p_total = median(dpois(total, lambda = total_obs))
  #p_total = sum((total < total_upper) & (total > total_lower))/dim(sim)[1]
  #p_total = sum(abs(total - sum(obs_vector)) < 1.96*sqrt(total))/dim(sim)[1]
  
  # probability of observed recurrent DNMs
  recurr = rowSums(sim > 1)
  p_recurr = median(dpois(recurr, lambda = total_recurr))
  #p_recurr = sum((recurr < recurr_upper) & (recurr > recurr_lower))/dim(sim)[1]
    #p_recurr = sum(abs(recurr - sum(obs_vector > 1)) < 1.96*sqrt(sum(obs_vector > 1)))/dim(sim)[1]

  # probability of zero with more than 3
  gws = rowSums(sim > 3)
  p_gws = median(dpois(gws, lambda = total_gws))
  #p_gws = sum(gws == sum(obs_vector > 3))/dim(sim)[1]

	store_likelihood[j, i] = p_total * p_recurr * p_gws
}


n_lof = seq(1, 2601, 20)
lof.multiplier = seq(0.0001,.1001, 0.001)

store_likelihood <-matrix(ncol=length(n_lof), nrow=length(lof.multiplier))
colnames(store_likelihood) = n_lof
rownames(store_likelihood) = lof.multiplier



#### actually_run default to FALSE because this takes a long time and needs a computing cluster - set to true to run again, otherwise loads from data

actually_run = FALSE

if (actually_run == TRUE) {
  for(i in seq(1,length(n_lof))) {
  
  	for(j in seq(1,length(lof.multiplier))) {
  		
  	  prob_recurr = mean(replicate(30, p_data_abc(n_lof[i], lof.multiplier[j], obs_vector)))
  		
  	  store_likelihood[j, i] = prob_recurr
  		
  	}
  	
  }

  store_likelihood_bf = store_likelihood/store_likelihood[1,1]
  
  library(reshape2)
  library(ggplot2)
  m = melt(store_likelihood_bf, varnames = c("lof.rate", "n_lof"), value.name = "probability_of_model")
  m$prop_lof = m$n_lof/max(m$n)
  
  ggplot(m) + geom_tile(aes(lof.rate, n_lof, fill = probability_of_model)) + 
    scale_fill_gradient(low = "white", high = "blue", guide = guide_colorbar(title = "Likelihood")) + 
    xlab("Proportion of Mutations\nResulting in Pathogenic Loss/Gain of Function") + 
    ylab("Number of Elements with\nMonoallelic Loss/Gain of Function Mechanism") +
    theme_bw(base_size = 18) +
    theme(strip.text = element_text(color="black"),strip.background = element_rect(fill="white", size=0),panel.border = element_blank()) + 
    theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())

  write.table(m, file = "../data/Figure4a_ML_model_bayes_factors.txt", col.names = TRUE, row.names = FALSE, sep = "\t", quote = FALSE)
}

```

```{r from remote}

m = read.table("../data/Figure4a_ML_model_bayes_factors.txt", header = TRUE, sep = "\t")

ggplot(m) + geom_tile(aes(lof.rate, n_lof, fill = probability_of_model)) +
  scale_fill_gradient(low = "white", high = "blue", guide = guide_colorbar(title = "Likelihood")) + 
  xlab("Proportion of Mutations\nResulting in Pathogenic Loss/Gain of Function") + 
  ylab("Number of Elements with\nMonoallelic Loss/Gain of Function Mechanism") +
  theme_bw(base_size = 18) +
  theme(strip.text = element_text(color="black"),strip.background = element_rect(fill="white", size=0),panel.border = element_blank()) + 
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())

# let's get the 5% cutoff for prob density
c = cumsum(m$probability_of_model[order(m$probability_of_model)])/sum(m$probability_of_model)

thresh100 = 0
thresh90 = m$probability_of_model[order(m$probability_of_model)][which.min(abs(c - 0.1))]
thresh50 = m$probability_of_model[order(m$probability_of_model)][which.min(abs(c - 0.5))]

ggplot(m) + geom_tile(aes(lof.rate, n_lof, fill = probability_of_model)) + geom_contour(aes(lof.rate, n_lof, z=probability_of_model),breaks=c(thresh90, thresh50),colour="red") +
    scale_fill_gradient(low = "white", high = "blue", guide = guide_colorbar(title = "Likelihood Ratio")) + 
    xlab("Proportion of Mutations\nResulting in Pathogenic Loss/Gain of Function") + 
    ylab("Number of Elements with\nMonoallelic Loss/Gain of Function Mechanism") +
    theme_bw(base_size = 18) +
    theme(strip.text = element_text(color="black"),strip.background = element_rect(fill="white", size=0),panel.border = element_blank()) + 
    theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank()) + ylim(0, max(m$n_lof)) + xlim(0, max(m$lof.rate)) + ggtitle("")


```

What proportion of the total number of variants are pathogenic under this model?

```{r add proportion lines}
prop = data.frame(lof.rate = c(0.003, 0.005, 0.01 , 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08))

n_elements_point05 = nrow(conserved_fb_active) * 0.0005/prop$lof.rate
n_elements_point1 = nrow(conserved_fb_active) * 0.001/prop$lof.rate
n_elements_point15 = nrow(conserved_fb_active) * 0.0015/prop$lof.rate
n_elements_point2 = nrow(conserved_fb_active) * 0.002/prop$lof.rate

#n_elements_1percent = nrow(conserved) * 0.01/prop$lof.rate

prop = data.frame(lof.rate = prop$lof.rate, n_elements = c(n_elements_point05, n_elements_point1, n_elements_point15, n_elements_point2), set = c(rep("0.05%", length(n_elements_point05)), rep("0.1%", length(n_elements_point1)), rep("0.15%", length(n_elements_point15)), rep("0.2%", length(n_elements_point2))))

ggplot(m) + geom_tile(aes(lof.rate, n_lof, fill = probability_of_model)) +
  scale_fill_gradient(low = "white", high = "blue", guide = guide_colorbar(title = "Bayes Factor")) + 
  xlab("Proportion of Mutations\nResulting in Pathogenic Loss/Gain of Function") + 
  ylab("Number of Elements with\nMonoallelic Loss/Gain of Function Mechanism") +
  theme_bw(base_size = 18) +
  theme(strip.text = element_text(color="black"),strip.background = element_rect(fill="white", size=0),panel.border = element_blank()) + 
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank()) +
  geom_line(data = prop, aes(lof.rate, n_elements, color = set)) + geom_point(data = prop, aes(lof.rate, n_elements, color = set)) + ylim(0, 2700) + scale_color_discrete(name = "% of all regulatory mutations\nwith monoallelic mechanism")

```

```{r marginals}

#number of elements
r = ddply(m, "lof.rate", function(df) data.frame(p = sum(df$probability_of_model)))
r$p = r$p/sum(r$p)

# later can split by coding/noncoding

ggplot(r) + geom_bar(aes(lof.rate, p), stat = "identity") +
  xlab("Proportion of Mutations\nResulting in Pathogenic Loss/Gain of Function") + 
  ylab("Probability Density") +
  theme_bw(base_size = 18) +
  theme(strip.text = element_text(color="black"),strip.background = element_rect(fill="white", size=0),panel.border = element_blank()) + 
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())


#number of elements
e = ddply(m, "n_lof", function(df) data.frame(p = sum(df$probability_of_model)))
e$p = e$p/sum(e$p)

# later can split by coding/noncoding

ggplot(e) + geom_bar(aes(n_lof, p), stat = "identity") +
  xlab("Number of Elements with Monoallelic LOF Mechanism") + 
  ylab("Probability Density") +
  theme_bw(base_size = 18) +
  theme(strip.text = element_text(color="black"),strip.background = element_rect(fill="white", size=0),panel.border = element_blank()) + 
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())

```


Using the data we have here, we want to make a genome-wide estimate of the contribution of de novo SNVs.

We will consider all of the non-coding elements here and split into quantiles of phastcons100 score:

```{r load elements and gencode transcripts}
library(phastCons100way.UCSC.hg19)
library(GenomicRanges)

conserved = read.table("../data/conserved_elements.min10_coverage.txt", header = TRUE, sep = "\t")
enhancers = read.table("../data/enhancer_elements.min10_coverage.txt", header = TRUE, sep = "\t")
heart = read.table("../data/heart_elements.min10_coverage.txt", header = TRUE, sep = "\t")
#control = read.table("../data/noncoding_control_elements.10bp_buffer.min10_coverage.30bp_element_minimum.30x_probe_coverage_minimum.no_ddg2p_overlap.txt", header = TRUE, sep = "\t")
#control = subset(control, (control$stop - control$start > 100))

all_elements = rbind(conserved, enhancers, heart)

gencode_v19_CDS = read.table("../data/gencode.v19.CDS.bed", header = FALSE, sep = "\t")
colnames(gencode_v19_CDS) = c("chr", "start", "stop", "chromHMM")


```

```{r load Roadmap fetal brain DHS data}

source("../R/mutation_null_model.R")
source("../R/annotation_tools.R")

male_fb_dhs = read.table(gzfile("../data/E081-DNase.hotspot.fdr0.01.broad.bed.gz"), header = FALSE, sep = "\t")
colnames(male_fb_dhs) = c("chr", "start", "stop", "id", "strength")
male_fb_chromHMM = read.table("../data/E081_15_coreMarks_mnemonics.bed", header = FALSE, sep = "\t")
colnames(male_fb_chromHMM) = c("chr", "start", "stop", "chromHMM")
male_fb_chromHMM = subset(male_fb_chromHMM, !(chromHMM %in% c("9_Het", "13_ReprPC", "14_ReprPCWk", "15_Quies")))

female_fb_dhs = read.table(gzfile("../data/E082-DNase.hotspot.fdr0.01.broad.bed.gz"), header = FALSE, sep = "\t")
colnames(female_fb_dhs) = c("chr", "start", "stop", "id", "strength")
female_fb_chromHMM = read.table("../data/E082_15_coreMarks_mnemonics.bed", header = FALSE, sep = "\t")
colnames(female_fb_chromHMM) = c("chr", "start", "stop", "chromHMM")
female_fb_chromHMM = subset(female_fb_chromHMM, !(chromHMM %in% c("9_Het", "13_ReprPC", "14_ReprPCWk", "15_Quies")))


# get the DDD specific fetal brain DHS

intersect_granges = function(b1, b2) {
  # intersect any two dataframes with chr, start, and stop
  b1 = GRanges(seqnames=Rle(b1$chr), ranges = IRanges(start = b1$start, end = b1$stop))
  b2 = GRanges(seqnames=Rle(b2$chr), ranges = IRanges(start = b2$start, end = b2$stop))
  i = intersect(b1, b2)
  
  new = data.frame(chr = as.character(i@seqnames),
                   start = as.integer(i@ranges@start),
                   stop = as.integer(i@ranges@start + i@ranges@width - 1))
  return(new)
}

difference_granges = function(b1, b2) {
  # take b1 and remove everything in b2
  b1 = GRanges(seqnames=Rle(b1$chr), ranges = IRanges(start = b1$start, end = b1$stop))
  b2 = GRanges(seqnames=Rle(b2$chr), ranges = IRanges(start = b2$start, end = b2$stop))
  i = setdiff(b1, b2)
  
  new = data.frame(chr = as.character(i@seqnames),
                   start = as.integer(i@ranges@start),
                   stop = as.integer(i@ranges@start + i@ranges@width - 1))
  return(new)
}

union_granges = function(b1, b2) {
  # intersect any two dataframes with chr, start, and stop
  b1 = GRanges(seqnames=Rle(b1$chr), ranges = IRanges(start = b1$start, end = b1$stop))
  b2 = GRanges(seqnames=Rle(b2$chr), ranges = IRanges(start = b2$start, end = b2$stop))
  i = union(b1, b2)
  
  new = data.frame(chr = as.character(i@seqnames),
                   start = as.integer(i@ranges@start),
                   stop = as.integer(i@ranges@start + i@ranges@width - 1))
  return(new)
}

fb_dhs = union_granges(male_fb_dhs, female_fb_dhs)
fb_dhs = difference_granges(fb_dhs, gencode_v19_CDS)

ddd_fb_dhs = intersect_granges(fb_dhs, all_elements)
ddd_fb_dhs$seq = as.character(get_sequence(ddd_fb_dhs$chr, ddd_fb_dhs$start, ddd_fb_dhs$stop))

ddd_fb_dhs$p_snp_null_no_methyl_correction = 2 * sapply(ddd_fb_dhs$seq, p_sequence)


### cut out and shift to mutation_rate_model.R into methylation_correction function at some point!
wgbs = read.table("../data/E008_WGBS_FractionalMethylation.bedGraph", header = FALSE, sep = "\t", stringsAsFactors = FALSE)
colnames(wgbs) = c("chr", "start", "stop", "prop_methylated")
wgbs$pos = wgbs$start + 1
wgbs = wgbs[,c("chr", "pos", "prop_methylated")]


wgbs = subset(wgbs, chr %in% paste0("chr", seq(1,22)))
wgbs$chr = factor(wgbs$chr, levels = paste0("chr", seq(1,22)), ordered = TRUE)

wgbs = filter_with_bed(wgbs, all_elements)
wgbs = get_region_id_multi_overlap(wgbs, all_elements)

#new version, fit to gnomAD non finnish european deep whole genomes (7,509)
methyl_df = read.table("../data/methylation_effect_on_obs_exp.ESC_sperm_matched_sites.gnomAD.txt", header = TRUE, sep = "\t")
methyl_df$prop_methylated = seq(0.025, 0.975, 0.05)
methylation_correction_model = lm(obs_exp_ratio ~ prop_methylated, methyl_df)


methyl_correction <- function(elements) {
  
  elements$region_id = paste0(elements$chr, ":", elements$start, "-", elements$stop)
  w = subset(wgbs, region_id %in% elements$region_id)
  
  wgbs_split = split(w, w$region_id)
  cpg_positions = sapply(wgbs_split, function(df) c(df$pos - elements$start[elements$region_id == df$region_id[1]] + 1))
  prop_methylated = sapply(wgbs_split, function(df) c(df$prop_methylated))
  
  seqs = elements$seq[match(names(prop_methylated), elements$region_id)]
  seqs = split(seqs, f = names(prop_methylated))
  
  p_uncorrected = elements$p_snp_null_no_methyl_correction[match(names(prop_methylated), elements$region_id)]

  p_corrected = mapply(p_sequence_meth, seqs, cpg_positions, prop_methylated, p_uncorrected, MoreArgs = list("correction_model" = methylation_correction_model))
  
  elements$p_snp_null[match(names(prop_methylated), elements$region_id)] = p_corrected
  
  return(elements)
}

###



ddd_fb_dhs = methyl_correction(ddd_fb_dhs)  # to do - add methyl correction code to the script

ddd_fb_dhs_intervals = GRanges(seqnames=ddd_fb_dhs$chr, IRanges(start = ddd_fb_dhs$start, width = ddd_fb_dhs$stop - ddd_fb_dhs$start + 1))
ddd_fb_dhs$phastcons100 = scores(phastCons100way.UCSC.hg19, ddd_fb_dhs_intervals)



hist(ddd_fb_dhs$phastcons100, xlab = "Evolutionary Conservation of DHS Peak (phastcons100)", main = "DDD phastcons distribution")

```

```{r plot observed v expected for each of the quantiles}

obs = read.table("../data/de_novos.ddd_8k.noncoding_included.2016-06-23.DHS_broad_peak_fdr_0.01.AllRoadmapTissues.txt", header = TRUE, sep = "\t")
obs = subset(obs, pp_dnm > 0.00781)
obs = subset(obs, nchar(as.character(ref)) == 1 & nchar(as.character(alt)) == 1)

blacklist = read.table("../data/all_stable_id_blacklist.txt", header = FALSE, sep = "\t")$V1

obs = subset(obs, !(person_stable_id %in% blacklist))

diagnosed = read.table("../data/ddd_8k.diagnosed.2016-06-23.txt", header = TRUE, sep = "\t")
diagnosed = subset(diagnosed, !(person_id %in% blacklist))
diagnosed_sim_ids = seq(1, length(unique(diagnosed$person_id)))
obs_diagnosed = subset(obs, person_stable_id %in% diagnosed$person_id)
obs_undiagnosed = subset(obs, !(person_stable_id %in% diagnosed$person_id))

# load the file indicating whether proband has neurodev disorder (also used to get number of probands on blacklist)
has_neurodev = read.table("../data/ddd_8k_probands.neurodev_terms.txt", header = TRUE, sep = "\t")
has_neurodev$has_neurodev_phenotype = ifelse(has_neurodev$has_neurodev_phenotype == "True", TRUE, FALSE)
n_children_removed = sum(has_neurodev$person_stable_id %in% blacklist)
has_neurodev = subset(has_neurodev, !(person_stable_id %in% blacklist))
has_neurodev$diagnosed = has_neurodev$person_stable_id %in% diagnosed$person_id

undiagnosed_neurodev = has_neurodev$person_stable_id[has_neurodev$has_neurodev_phenotype & !has_neurodev$diagnosed]

o = subset(obs, person_stable_id %in% undiagnosed_neurodev)

library(plyr)

ddd_phastcons_q = unique(quantile(ddd_fb_dhs$phastcons100, seq(0, 1, length.out = 7)))
ddd_fb_dhs$phastcons_quantile = cut(ddd_fb_dhs$phastcons100, ddd_phastcons_q, include.lowest = TRUE)
ddd = ddply(ddd_fb_dhs, "phastcons_quantile", function(df) data.frame(expected = sum(df$p_snp_null)*length(undiagnosed_neurodev), observed = nrow(filter_with_bed(o, df))))
ddd$mid = (ddd_phastcons_q[1:(length(ddd_phastcons_q)-1)] + ddd_phastcons_q[2:(length(ddd_phastcons_q))])/2
ddd$ratio = ddd$observed/ddd$expected


```

Sliding window approach:

```{r sliding window approach}

obs_expected_df <- function(phastcons_low, phastcons_high, obs, elements) {
  elements = subset(elements, (phastcons100 >= phastcons_low) & (phastcons100 <= phastcons_high))
  return(data.frame(phastcons_low = phastcons_low, phastcons_high = phastcons_high, mid = (phastcons_low + phastcons_high)/2, expected = sum(elements$p_snp_null)*length(undiagnosed_neurodev), observed = nrow(filter_with_bed(obs, elements))))
}

obs_expected_df <- function(a, b, obs, elements) {
  elements = elements[order(elements$phastcons100),]
  elements = elements[a:b,]
  return(data.frame(mid = median(elements$phastcons100), expected = sum(elements$p_snp_null)*length(undiagnosed_neurodev), observed = nrow(filter_with_bed(obs, elements))))
}

n = 1000
j = 100
starts = seq(1, nrow(ddd_fb_dhs) - n - 1, by = j)
stops = seq(n, nrow(ddd_fb_dhs), by = j)

ddd = do.call(rbind,mapply(obs_expected_df, starts, stops, MoreArgs = list(obs = o, elements = ddd_fb_dhs), SIMPLIFY = FALSE))

ddd$ratio = ddd$observed/ddd$expected

```

Now, estimate the genome-wide mutability in each of these categories.

```{r genome-wide mutability}

set.seed(42)  # we pick 1000 randomly sampled sequences in because generating phastcons and mutability is slow
sample_size = 1000

multiplier = nrow(fb_dhs)/sample_size
fb_dhs_sample = fb_dhs[sample(seq(1, nrow(fb_dhs)), sample_size, replace = FALSE),]

fb_dhs_sample$seq = as.character(get_sequence(fb_dhs_sample$chr, fb_dhs_sample$start, fb_dhs_sample$stop))
fb_dhs_sample$p_snp_null = 2 * sapply(fb_dhs_sample$seq, p_sequence)

wg = GRanges(seqnames=fb_dhs_sample$chr, IRanges(start = fb_dhs_sample$start, width = fb_dhs_sample$stop - fb_dhs_sample$start + 1))
fb_dhs_sample$phastcons100 = scores(phastCons100way.UCSC.hg19, wg)

fb_dhs_sample$phastcons_quantile = cut(fb_dhs_sample$phastcons100, ddd_phastcons_q, include.lowest = TRUE)

wg = ddply(fb_dhs_sample, "phastcons_quantile", function(df) data.frame(expected = sum(df$p_snp_null)*length(undiagnosed_neurodev)*multiplier))

hist(fb_dhs_sample$phastcons100, xlab = "Evolutionary Conservation of DHS Peak (phastcons100)", main = "Whole Genome phastcons distribution")

# plot distribution in each bin
fb_dhs_sample$phastcons_bins = cut(fb_dhs_sample$phastcons100, seq(0,1,0.1), include.lowest = TRUE)
megabase = ddply(fb_dhs_sample, "phastcons_bins", function(df) data.frame(megabase = sum(df$stop - df$start)*multiplier/1e6))

barplot(megabase$megabase)
```


Using logistic regression:

```{r logit estimate}
d = ddd
d$excess = d$ratio - 1
d$excess[d$excess < 0] = 0

fit <- glm(excess/max(d$excess) ~ mid, data=d, family=binomial())
fit_pred = predict(fit, data.frame(mid = seq(0,1,0.01)), type = "response", se.fit = TRUE)
fit_lower <- approxfun(seq(0,1,0.01), 1 + (fit_pred$fit - 1.96*fit_pred$se.fit)*max(d$excess))
fit_upper <- approxfun(seq(0,1,0.01), 1 + (fit_pred$fit + 1.96*fit_pred$se.fit)*max(d$excess))

plot(d$mid, d$ratio, xlim = c(0, 1.0), ylim = c(0.5, 1.5), ylab = "Observed/Expected in DDD", xlab = "Evolutionary Conservation of DHS peak (phastcons100)", pch=16, main = "DNM enrichment across evolutionary conservation spectrum\n")

fit_plot = predict(fit, data.frame(mid = seq(0,1,0.01)), type = "response", se.fit = TRUE)
# for (i in seq(1,nrow(d))) {
#   lines(x = c(d$mid[i], d$mid[i]), y = c(d$observed[i]/(d$expected[i] - 2*sqrt(d$expected[i])), d$observed[i]/(d$expected[i] + 2*sqrt(d$expected[i]))))
# }
lines(seq(0,1,0.01), 1 + fit_plot$fit*max(d$excess))
lines(seq(0,1,0.01), 1 + (fit_plot$fit + 1.96*fit_plot$se.fit)*max(d$excess))
lines(seq(0,1,0.01), 1 + (fit_plot$fit - 1.96*fit_plot$se.fit)*max(d$excess))
abline(h = 1.0)

fb_dhs_sample$predicted_excess = predict(fit, data.frame(mid = fb_dhs_sample$phastcons100), type = "response")*max(d$excess)
fb_dhs_sample$predicted_excess[fb_dhs_sample$predicted_excess < 0] = 0

fb_dhs_sample$predicted_excess_lower = fit_lower(fb_dhs_sample$phastcons100) - 1
fb_dhs_sample$predicted_excess_lower[fb_dhs_sample$predicted_excess_lower < 0] = 0

fb_dhs_sample$predicted_excess_upper = fit_upper(fb_dhs_sample$phastcons100) - 1
fb_dhs_sample$predicted_excess_upper[fb_dhs_sample$predicted_excess_upper < 0] = 0

total_predicted = sum(fb_dhs_sample$p_snp_null*length(undiagnosed_neurodev)*fb_dhs_sample$predicted_excess)*multiplier
total_predicted_lower = sum(fb_dhs_sample$p_snp_null*length(undiagnosed_neurodev)*fb_dhs_sample$predicted_excess_lower)*multiplier
total_predicted_upper = sum(fb_dhs_sample$p_snp_null*length(undiagnosed_neurodev)*fb_dhs_sample$predicted_excess_upper)*multiplier

```

Comparing non-coding burden to coding from McRae et. al, 2017:

```{r comparing to coding}

mcrae_pop_size = 4293
short_pop_size = 7930

missense_excess = round(1220 * short_pop_size/mcrae_pop_size)/short_pop_size
ptv_excess = round(576 * short_pop_size/mcrae_pop_size)/short_pop_size
noncoding_excess = round(total_predicted)/short_pop_size

df = data.frame(excess = c(noncoding_excess, ptv_excess, missense_excess, 1-(noncoding_excess + ptv_excess + missense_excess)), set = c("non-coding SNV", "PTV", "missense", "unexplained"))
df$set = factor(df$set, levels = df$set, ordered = TRUE)

ggplot(df) + geom_bar(aes(set, excess, fill = set), stat = "identity") + xlab("Variant Class") + 
  ylab("Proportion of DDD probands") +
  theme_bw(base_size = 18) +
  theme(strip.text = element_text(color="black"),strip.background = element_rect(fill="white", size=0),panel.border = element_blank(), legend.title = element_blank()) + guides(fill = FALSE) +
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())



```
